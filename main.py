#!/usr/bin/env python3
"""
ç»Ÿä¸€å†…å®¹æŠ“å–APIæœåŠ¡å™¨
æä¾›è´´å§å’Œå¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–æœåŠ¡
å®Œå…¨ä¿ç•™åŸæœ‰ä»£ç é€»è¾‘,åªåšè·¯ç”±åˆå¹¶
"""

import time
import json
import os
import re
from datetime import datetime
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import traceback
from urllib.parse import urlparse, parse_qs
import urllib.request
import logging
from logging.handlers import RotatingFileHandler

# å¯¼å…¥åŸå§‹çˆ¬è™«ç±»çš„ä¾èµ–
from playwright.sync_api import sync_playwright
import requests
from bs4 import BeautifulSoup

# å»æ°´å°åŠŸèƒ½çš„å¯¼å…¥
try:
    import cv2
    import numpy as np
    HAS_CV2 = True
except ImportError:
    HAS_CV2 = False
    print("âš ï¸ OpenCVæœªå®‰è£…,å°†è·³è¿‡å»æ°´å°åŠŸèƒ½ã€‚å¦‚éœ€å»æ°´å°è¯·å®‰è£…: pip install opencv-python")

app = Flask(__name__)
CORS(app)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if not os.path.exists('logs'):
    os.makedirs('logs')
    
file_handler = RotatingFileHandler('logs/unified_api.log', maxBytes=10240000, backupCount=10)
file_handler.setFormatter(logging.Formatter(
    '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
))
file_handler.setLevel(logging.INFO)
app.logger.addHandler(file_handler)


# ============================================================================
# è´´å§æŠ“å–ç±» - å®Œå…¨ä¿ç•™åŸæœ‰é€»è¾‘
# ============================================================================
class TiebaPostScraperAPI:
    def __init__(self, remove_watermarks=True):
        # APIæœåŠ¡å™¨çš„å·¥ä½œç›®å½•
        self.work_dir = os.path.abspath(os.path.dirname(__file__))
        self.static_dir = os.path.join(self.work_dir, 'static', 'tieba')
        self.images_dir = os.path.join(self.static_dir, 'images')
        self.posts_dir = os.path.join(self.static_dir, 'posts')
        
        # å»æ°´å°åŠŸèƒ½å¼€å…³
        self.remove_watermarks = remove_watermarks and HAS_CV2
        
        self.ensure_directories()
        
        if self.remove_watermarks:
            logger.info("ğŸ¨ å·²å¯ç”¨è‡ªåŠ¨å»æ°´å°åŠŸèƒ½")
        elif remove_watermarks and not HAS_CV2:
            logger.warning("âš ï¸ å»æ°´å°åŠŸèƒ½éœ€è¦OpenCV,è¯·è¿è¡Œ: pip install opencv-python")
        else:
            logger.info("ğŸ“· æœªå¯ç”¨å»æ°´å°åŠŸèƒ½")
    
    def ensure_directories(self):
        """ç¡®ä¿æ‰€éœ€ç›®å½•å­˜åœ¨"""
        for directory in [self.static_dir, self.images_dir, self.posts_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)
                logger.info(f"âœ… åˆ›å»ºç›®å½•: {directory}")
    
    def clean_old_files(self, max_age_hours=24):
        """æ¸…ç†è¶…è¿‡æŒ‡å®šæ—¶é—´çš„æ—§æ–‡ä»¶"""
        try:
            current_time = time.time()
            max_age_seconds = max_age_hours * 3600
            
            for directory in [self.images_dir, self.posts_dir]:
                for root, dirs, files in os.walk(directory):
                    for file in files:
                        file_path = os.path.join(root, file)
                        if current_time - os.path.getctime(file_path) > max_age_seconds:
                            try:
                                os.remove(file_path)
                                logger.info(f"ğŸ—‘ï¸ æ¸…ç†æ—§æ–‡ä»¶: {file_path}")
                            except Exception as e:
                                logger.error(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    
                    # æ¸…ç†ç©ºç›®å½•
                    for dir_name in dirs:
                        dir_path = os.path.join(root, dir_name)
                        try:
                            if not os.listdir(dir_path):
                                os.rmdir(dir_path)
                                logger.info(f"ğŸ—‘ï¸ æ¸…ç†ç©ºç›®å½•: {dir_path}")
                        except Exception:
                            pass
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def extract_post_id(self, url):
        """ä»è´´å§é“¾æ¥ä¸­æå–å¸–å­ID"""
        try:
            patterns = [
                r'/p/(\d+)',  # æ ‡å‡†æ ¼å¼
                r'tid=(\d+)',  # å‚æ•°æ ¼å¼
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    return match.group(1)
            
            return None
        except Exception as e:
            logger.error(f"âŒ è§£æURLå¤±è´¥: {e}")
            return None
    
    def clean_tieba_url(self, url):
        """æ¸…ç†è´´å§URL,å»é™¤å¤šä½™å‚æ•°"""
        try:
            post_id = self.extract_post_id(url)
            if post_id:
                clean_url = f"https://tieba.baidu.com/p/{post_id}"
                return clean_url
            return url
        except:
            return url
    
    def remove_watermark(self, image_path):
        """å»é™¤å›¾ç‰‡å³ä¸‹è§’æ°´å°"""
        if not HAS_CV2:
            logger.warning(f"âš ï¸ è·³è¿‡å»æ°´å° (OpenCVæœªå®‰è£…): {os.path.basename(image_path)}")
            return False
        
        try:
            image = cv2.imread(image_path)
            if image is None:
                logger.error(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {image_path}")
                return False
            
            height, width = image.shape[:2]
            mask = np.zeros(image.shape[:2], np.uint8)
            
            watermark_width = min(420, int(width * 0.3))
            watermark_height = min(50, int(height * 0.1))
            
            cv2.rectangle(mask, 
                         (width - watermark_width, height - watermark_height), 
                         (width, height), 
                         255, -1)
            
            denoised_image = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)
            success = cv2.imwrite(image_path, denoised_image)
            
            if success:
                logger.info(f"âœ… å·²å»æ°´å°: {os.path.basename(image_path)}")
                return True
            else:
                logger.error(f"âŒ ä¿å­˜å»æ°´å°å›¾ç‰‡å¤±è´¥: {os.path.basename(image_path)}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å»æ°´å°å¤„ç†å¤±è´¥ {os.path.basename(image_path)}: {e}")
            return False
    
    def download_images(self, images, post_id):
        """ä¸‹è½½å¸–å­ä¸­çš„å›¾ç‰‡åˆ°é™æ€ç›®å½•"""
        if not images:
            return []
        
        # åˆ›å»ºå¸–å­ä¸“å±çš„å›¾ç‰‡ç›®å½•
        post_images_dir = os.path.join(self.images_dir, post_id)
        if not os.path.exists(post_images_dir):
            os.makedirs(post_images_dir)
        
        downloaded_images = []
        
        for i, img in enumerate(images, 1):
            try:
                img_url = img['src']
                if not img_url:
                    continue
                
                # ç¡®å®šæ–‡ä»¶æ‰©å±•å
                if 'jpeg' in img_url.lower() or 'jpg' in img_url.lower():
                    ext = '.jpg'
                elif 'png' in img_url.lower():
                    ext = '.png'
                elif 'gif' in img_url.lower():
                    ext = '.gif'
                elif 'webp' in img_url.lower():
                    ext = '.webp'
                else:
                    ext = '.jpg'  # é»˜è®¤
                
                img_filename = f"image_{i:03d}{ext}"
                img_filepath = os.path.join(post_images_dir, img_filename)
                
                # ä¸‹è½½å›¾ç‰‡
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://tieba.baidu.com/'
                }
                
                req = urllib.request.Request(img_url, headers=headers)
                with urllib.request.urlopen(req, timeout=15) as response:
                    if response.status == 200:
                        with open(img_filepath, 'wb') as f:
                            f.write(response.read())
                        
                        # å»æ°´å°å¤„ç†
                        watermark_removed = False
                        if self.remove_watermarks:
                            watermark_removed = self.remove_watermark(img_filepath)
                        
                        # ç”Ÿæˆå¯è®¿é—®çš„URL
                        image_url = f"/tieba/images/{post_id}/{img_filename}"
                        
                        downloaded_images.append({
                            'original_url': img_url,
                            'local_path': img_filepath,
                            'filename': img_filename,
                            'image_url': image_url,
                            'alt': img.get('alt', ''),
                            'title': img.get('title', ''),
                            'watermark_removed': watermark_removed
                        })
                        logger.info(f"ğŸ“· å·²ä¸‹è½½å›¾ç‰‡ {i}: {img_filename}")
                    else:
                        logger.warning(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ {i}: HTTP {response.status}")
                        
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½å›¾ç‰‡ {i} å¤±è´¥: {e}")
                continue
        
        return downloaded_images
    
    def scrape_tieba_post(self, post_url):
        """æŠ“å–è´´å§å¸–å­å†…å®¹"""
        logger.info(f"ğŸ” å¼€å§‹æŠ“å–è´´å§å¸–å­: {post_url}")
        
        clean_url = self.clean_tieba_url(post_url)
        logger.info(f"ğŸ”— æ¸…ç†åçš„URL: {clean_url}")
        
        # ç›´æ¥ä½¿ç”¨æµè§ˆå™¨æ–¹å¼
        logger.info("ğŸŒ ä½¿ç”¨æµè§ˆå™¨æ–¹å¼æŠ“å–...")
        return self.scrape_with_browser(clean_url)
    
    def scrape_with_browser(self, post_url):
        """ä½¿ç”¨æµè§ˆå™¨æŠ“å–"""
        logger.info("ğŸŒ å¯åŠ¨æµè§ˆå™¨è¿›è¡ŒæŠ“å–...")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-extensions',
                    '--disable-plugins',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--no-first-run',
                    '--no-default-browser-check',
                    '--single-process',
                ]
            )
            
            context = browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0.0.0 Safari/537.36"
                ),
                viewport={"width": 1366, "height": 768},
            )
            
            page = context.new_page()
            
            try:
                logger.info("ğŸŒ æ­£åœ¨è®¿é—®å¸–å­é¡µé¢...")
                page.set_default_timeout(45000)
                
                # é¢„çƒ­è®¿é—®
                try:
                    page.goto("https://www.baidu.com", wait_until="domcontentloaded", timeout=30000)
                    time.sleep(3)
                except:
                    pass
                
                # è®¿é—®ç›®æ ‡é¡µé¢
                response = page.goto(post_url, wait_until="domcontentloaded", timeout=45000)
                logger.info(f"ğŸ“„ é¡µé¢å“åº”çŠ¶æ€: {response.status if response else 'æ— å“åº”'}")
                
                time.sleep(3)
                
                page_title = page.title()
                page_url = page.url
                logger.info(f"ğŸ“° é¡µé¢æ ‡é¢˜: {page_title}")
                logger.info(f"ğŸ”— æœ€ç»ˆURL: {page_url}")
                
                # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
                try:
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(2)
                except:
                    pass
                
                # è·å–é¡µé¢å†…å®¹å¹¶è§£æ
                page_content = page.content()
                return self.parse_html_content(page_content, post_url)
                
            except Exception as e:
                logger.error(f"âŒ æµè§ˆå™¨æŠ“å–å¤±è´¥: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'url': post_url,
                    'method': 'browser_extraction'
                }
            finally:
                browser.close()
    
    def parse_html_content(self, html_content, post_url):
        """è§£æHTMLå†…å®¹ - å®Œå…¨ä¿ç•™åŸæœ‰é€»è¾‘"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–å¸–å­åŸºæœ¬ä¿¡æ¯
            post_info = self.extract_post_info(soup, post_url)
            
            # æå–ä¸»å¸–å†…å®¹
            main_post = self.extract_main_post(soup)
            
            # æå–å›å¤å†…å®¹
            replies = self.extract_replies(soup)
            
            # æ”¶é›†æ‰€æœ‰å›¾ç‰‡
            all_images = []
            if main_post.get('images'):
                all_images.extend(main_post['images'])
            
            for reply in replies:
                if reply.get('images'):
                    all_images.extend(reply['images'])
            
            logger.info(f"âœ… HTMLè§£ææˆåŠŸ: ä¸»å¸–+{len(replies)}æ¡å›å¤, {len(all_images)}å¼ å›¾ç‰‡")
            
            # ç”ŸæˆMarkdown
            main_markdown = self.generate_main_markdown(post_info, main_post)
            comments_markdown = self.generate_comments_markdown(post_info, replies)
            
            return {
                'success': True,
                'post_info': post_info,
                'main_post': main_post,
                'replies': replies,
                'total_replies': len(replies),
                'all_images': all_images,
                'total_images': len(all_images),
                'main_markdown': main_markdown,
                'comments_markdown': comments_markdown,
                'char_count': len(main_markdown) + len(comments_markdown),
                'method': 'html_parsing',
                'extraction_time': datetime.now().isoformat(),
                'url': post_url
            }
            
        except Exception as e:
            logger.error(f"âŒ HTMLè§£æå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'url': post_url,
                'method': 'html_parsing'
            }
    
    def extract_post_info(self, soup, post_url):
        """æå–å¸–å­åŸºæœ¬ä¿¡æ¯"""
        post_info = {
            'title': 'æœªæ‰¾åˆ°æ ‡é¢˜',
            'author': 'æœªçŸ¥ä½œè€…',
            'post_time': 'æœªçŸ¥æ—¶é—´',
            'forum_name': 'æœªçŸ¥è´´å§',
            'post_id': self.extract_post_id(post_url),
            'url': post_url
        }
        
        # æå–æ ‡é¢˜
        title_selectors = [
            '.core_title_txt',
            '.p_title',
            'h1.core_title_txt',
            'h3.core_title_txt',
            '[class*="title"]'
        ]
        
        for selector in title_selectors:
            try:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text().strip()
                    if title_text and len(title_text) > 3:
                        post_info['title'] = title_text
                        logger.info(f"âœ… æ‰¾åˆ°æ ‡é¢˜: {title_text}")
                        break
            except:
                continue
        
        # æå–è´´å§åç§°
        forum_selectors = [
            '.card_title',
            '.forum_name',
            'a[href*="/f?kw="]'
        ]
        
        for selector in forum_selectors:
            try:
                forum_elem = soup.select_one(selector)
                if forum_elem:
                    forum_text = forum_elem.get_text().strip()
                    if forum_text and len(forum_text) > 1:
                        post_info['forum_name'] = forum_text.replace('å§', '')
                        logger.info(f"âœ… æ‰¾åˆ°è´´å§: {forum_text}")
                        break
            except:
                continue
        
        return post_info
    
    def extract_main_post(self, soup):
        """æå–ä¸»å¸–å†…å®¹"""
        main_post = {
            'author': 'æœªçŸ¥ä½œè€…',
            'post_time': 'æœªçŸ¥æ—¶é—´',
            'content': [],
            'images': []
        }
        
        # æŸ¥æ‰¾ä¸»å¸–å®¹å™¨
        main_post_selectors = [
            '.d_post_content',
            '.p_postlist .l_post:first-child',
            '.core_reply_wrapper .l_post:first-child'
        ]
        
        main_post_elem = None
        for selector in main_post_selectors:
            try:
                main_post_elem = soup.select_one(selector)
                if main_post_elem:
                    logger.info(f"âœ… æ‰¾åˆ°ä¸»å¸–å®¹å™¨: {selector}")
                    break
            except:
                continue
        
        if main_post_elem:
            # æå–ä½œè€…
            author_selectors = [
                '.p_author_name',
                '.username',
                'a[username]'
            ]
            
            for selector in author_selectors:
                try:
                    author_elem = main_post_elem.select_one(selector)
                    if author_elem:
                        author_text = author_elem.get_text().strip()
                        if author_text:
                            main_post['author'] = author_text
                            logger.info(f"âœ… ä¸»å¸–ä½œè€…: {author_text}")
                            break
                except:
                    continue
            
            # æå–å†…å®¹å’Œå›¾ç‰‡
            main_post['content'], main_post['images'], main_post['content_elements'] = self.extract_post_content(main_post_elem)
        
        return main_post
    
    def extract_replies(self, soup):
        """æå–å›å¤å†…å®¹"""
        replies = []
        
        reply_selectors = [
            '.l_post[data-field*="content"]',
            '.core_reply .l_post'
        ]
        
        reply_elements = []
        for selector in reply_selectors:
            try:
                elements = soup.select(selector)
                if elements and len(elements) > 1:
                    reply_elements = elements[1:]  # è·³è¿‡ç¬¬ä¸€ä¸ª(ä¸»å¸–)
                    logger.info(f"âœ… æ‰¾åˆ° {len(reply_elements)} æ¡å›å¤")
                    break
            except:
                continue
        
        for i, reply_elem in enumerate(reply_elements[:10], 1):  # é™åˆ¶å‰10æ¡å›å¤
            try:
                reply_data = {
                    'floor': i + 1,
                    'author': 'æœªçŸ¥ç”¨æˆ·',
                    'post_time': 'æœªçŸ¥æ—¶é—´',
                    'content': [],
                    'images': []
                }
                
                # æå–å›å¤ä½œè€…
                author_selectors = [
                    '.p_author_name',
                    '.username',
                    'a[username]'
                ]
                
                for selector in author_selectors:
                    try:
                        author_elem = reply_elem.select_one(selector)
                        if author_elem:
                            author_text = author_elem.get_text().strip()
                            if author_text:
                                reply_data['author'] = author_text
                                break
                    except:
                        continue
                
                # æå–å›å¤å†…å®¹å’Œå›¾ç‰‡
                reply_data['content'], reply_data['images'], reply_data['content_elements'] = self.extract_post_content(reply_elem)
                
                if reply_data['content'] or reply_data['images']:
                    replies.append(reply_data)
                    logger.info(f"ğŸ“ å›å¤ {i}: {reply_data['author']} - {len(reply_data['content'])}æ®µè½, {len(reply_data['images'])}å›¾ç‰‡")
                
            except Exception as e:
                logger.error(f"âš ï¸ å¤„ç†å›å¤ {i} å¤±è´¥: {e}")
                continue
        
        return replies
    
    def extract_post_content(self, post_elem):
        """æŒ‰åŸå§‹é¡ºåºæå–å¸–å­å†…å®¹(æ–‡æœ¬å’Œå›¾ç‰‡æ··åˆ) - å®Œå…¨ä¿ç•™åŸæœ‰é€»è¾‘"""
        content_elements = []  # å­˜å‚¨æ··åˆçš„å†…å®¹å…ƒç´ 
        
        # æŸ¥æ‰¾å†…å®¹å®¹å™¨
        content_selectors = [
            '.d_post_content',
            '.p_content',
            '.post_content',
            '.content'
        ]
        
        content_elem = None
        for selector in content_selectors:
            try:
                content_elem = post_elem.select_one(selector)
                if content_elem:
                    break
            except:
                continue
        
        if not content_elem:
            content_elem = post_elem
        
        # æŒ‰åŸå§‹DOMé¡ºåºéå†æ‰€æœ‰å­å…ƒç´ 
        processed_texts = set()
        
        # ç‰¹æ®Šå¤„ç†è´´å§çš„HTMLç»“æ„ - ä½¿ç”¨æ›´ç®€å•ç›´æ¥çš„æ–¹æ³•
        # ç›´æ¥å¤„ç†HTMLå†…å®¹,æŒ‰<br>åˆ†éš”æ–‡æœ¬,ä¿æŒå›¾ç‰‡ä½ç½®
        content_html = str(content_elem)
        
        # å…ˆæå–æ‰€æœ‰æ–‡æœ¬,æŒ‰<br>åˆ†å‰²
        import re
        # å°†<br>æ›¿æ¢ä¸ºç‰¹æ®Šåˆ†éš”ç¬¦
        text_content = re.sub(r'<br[^>]*>', '|||BR|||', content_html)
        # ç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾,ä¿ç•™æ–‡æœ¬
        text_content = re.sub(r'<[^>]+>', '', text_content)
        # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²
        text_parts = text_content.split('|||BR|||')
        
        # åŒæ—¶æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
        img_elements = content_elem.find_all('img')
        
        # æ··åˆå¤„ç†æ–‡æœ¬å’Œå›¾ç‰‡
        img_index = 0
        for part in text_parts:
            part = part.strip()
            if part and self._is_valid_text_content(part, processed_texts):
                clean_text = self._clean_text_content(part)
                if clean_text and clean_text not in processed_texts:
                    text_info = {
                        'type': 'text',
                        'content': clean_text,
                        'tag': 'text'
                    }
                    content_elements.append(text_info)
                    processed_texts.add(clean_text)
                    logger.info(f"ğŸ“ å‘ç°æ–‡æœ¬: {clean_text[:50]}...")
            
            # åœ¨æ¯ä¸ªæ–‡æœ¬æ®µè½åå¯èƒ½æœ‰å›¾ç‰‡
            if img_index < len(img_elements):
                img = img_elements[img_index]
                img_src = img.get('src') or img.get('data-original') or img.get('original')
                if img_src:
                    # å¤„ç†ç›¸å¯¹URL
                    if img_src.startswith('//'):
                        img_src = 'https:' + img_src
                    elif img_src.startswith('/'):
                        img_src = 'https://tieba.baidu.com' + img_src
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ç™¾åº¦å›¾ç‰‡URL
                    if ('baidu.com' in img_src and (
                        'imgsrc.baidu.com' in img_src or 
                        'hiphotos.baidu.com' in img_src or 
                        'tiebapic.baidu.com' in img_src)) or 'BDE_Image' in img.get('class', []):
                        
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡è¿™ä¸ªå›¾ç‰‡
                        img_already_added = any(
                            el['type'] == 'image' and el['src'] == img_src 
                            for el in content_elements
                        )
                        
                        if not img_already_added:
                            img_info = {
                                'type': 'image',
                                'src': img_src,
                                'alt': img.get('alt', ''),
                                'title': img.get('title', '')
                            }
                            content_elements.append(img_info)
                            logger.info(f"ğŸ“· å‘ç°å›¾ç‰‡: {img_src}")
                            img_index += 1
        
        # å¤„ç†å‰©ä½™çš„å›¾ç‰‡
        while img_index < len(img_elements):
            img = img_elements[img_index]
            img_src = img.get('src') or img.get('data-original') or img.get('original')
            if img_src:
                if img_src.startswith('//'):
                    img_src = 'https:' + img_src
                elif img_src.startswith('/'):
                    img_src = 'https://tieba.baidu.com' + img_src
                
                if ('baidu.com' in img_src and (
                    'imgsrc.baidu.com' in img_src or 
                    'hiphotos.baidu.com' in img_src or 
                    'tiebapic.baidu.com' in img_src)) or 'BDE_Image' in img.get('class', []):
                    
                    img_already_added = any(
                        el['type'] == 'image' and el['src'] == img_src 
                        for el in content_elements
                    )
                    
                    if not img_already_added:
                        img_info = {
                            'type': 'image',
                            'src': img_src,
                            'alt': img.get('alt', ''),
                            'title': img.get('title', '')
                        }
                        content_elements.append(img_info)
                        logger.info(f"ğŸ“· å‘ç°å›¾ç‰‡: {img_src}")
            img_index += 1
        
        # å¦‚æœä¸Šé¢çš„æ–¹æ³•æ²¡æœ‰æ‰¾åˆ°å†…å®¹,ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        if not content_elements:
            all_elements = content_elem.find_all(['p', 'div', 'span', 'img', 'br'], recursive=True)
            if not all_elements:
                all_elements = [content_elem]
            
            for elem in all_elements:
                try:
                    # å¤„ç†å›¾ç‰‡å…ƒç´ 
                    if elem.name == 'img':
                        img_src = elem.get('src') or elem.get('data-original') or elem.get('original')
                        if img_src:
                            # å¤„ç†ç›¸å¯¹URL
                            if img_src.startswith('//'):
                                img_src = 'https:' + img_src
                            elif img_src.startswith('/'):
                                img_src = 'https://tieba.baidu.com' + img_src
                        
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ç™¾åº¦å›¾ç‰‡URL
                            if ('baidu.com' in img_src and (
                                'imgsrc.baidu.com' in img_src or 
                                'hiphotos.baidu.com' in img_src or 
                                'tiebapic.baidu.com' in img_src)) or 'BDE_Image' in elem.get('class', []):
                                
                                # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡è¿™ä¸ªå›¾ç‰‡
                                img_already_added = any(
                                    el['type'] == 'image' and el['src'] == img_src 
                                    for el in content_elements
                                )
                                
                                if not img_already_added:
                                    img_info = {
                                        'type': 'image',
                                        'src': img_src,
                                        'alt': elem.get('alt', ''),
                                        'title': elem.get('title', '')
                                    }
                                    content_elements.append(img_info)
                                    logger.info(f"ğŸ“· å‘ç°å›¾ç‰‡: {img_src}")
                    
                    # å¤„ç†æ–‡æœ¬å…ƒç´ 
                    elif elem.name in ['p', 'div', 'span']:
                        # è·³è¿‡åŒ…å«å›¾ç‰‡çš„å…ƒç´ ,é¿å…é‡å¤
                        if elem.find('img'):
                            continue
                        
                        text = elem.get_text(strip=True)
                        
                        # æ¸…ç†å’Œè¿‡æ»¤æ–‡æœ¬
                        if self._is_valid_text_content(text, processed_texts):
                            # è¿›ä¸€æ­¥æ¸…ç†æ–‡æœ¬
                            clean_text = self._clean_text_content(text)
                            if clean_text and clean_text not in processed_texts:
                                text_info = {
                                    'type': 'text',
                                    'content': clean_text,
                                    'tag': elem.name
                                }
                                content_elements.append(text_info)
                                processed_texts.add(clean_text)
                                logger.info(f"ğŸ“ å‘ç°æ–‡æœ¬: {clean_text[:50]}...")
                                
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†å…ƒç´ å¤±è´¥: {e}")
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹,å°è¯•å¤‡ç”¨æ–¹æ³•
        if not content_elements:
            content_elements = self._fallback_content_extraction(content_elem, processed_texts)
        
        # åˆ†ç¦»ä¸ºä¼ ç»Ÿæ ¼å¼ä»¥ä¿æŒå‘åå…¼å®¹
        content_paragraphs = [elem['content'] for elem in content_elements if elem['type'] == 'text']
        images = [elem for elem in content_elements if elem['type'] == 'image']
        
        return content_paragraphs, images, content_elements
    
    def _fallback_content_extraction(self, content_elem, processed_texts):
        """å¤‡ç”¨å†…å®¹æå–æ–¹æ³•"""
        content_elements = []
        logger.info("ä½¿ç”¨å¤‡ç”¨å†…å®¹æå–æ–¹æ³•...")
        
        # æ–¹æ³•1:ç›´æ¥è·å–æ–‡æœ¬å¹¶æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²
        direct_text = content_elem.get_text(separator=' ', strip=True)
        
        if direct_text and len(direct_text) > 20:
            logger.info(f"å¤‡ç”¨æ–¹æ³•è·å–åˆ°æ–‡æœ¬é•¿åº¦: {len(direct_text)}")
            # åˆ†å‰²æˆæ®µè½ - ä½¿ç”¨å¤šç§åˆ†éš”ç¬¦
            sentences = []
            for delimiter in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n\n', '\r\n']:
                if delimiter in direct_text:
                    sentences = direct_text.split(delimiter)
                    break
            
            if not sentences:
                sentences = [direct_text]  # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦,ä½¿ç”¨æ•´æ®µæ–‡æœ¬
            
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 5:  # é™ä½é•¿åº¦è¦æ±‚
                    # æ·»åŠ ç»“æŸç¬¦å·(å¦‚æœéœ€è¦çš„è¯)
                    if not sentence.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                        sentence += 'ã€‚'
                    
                    if self._is_valid_text_content(sentence, processed_texts):
                        clean_text = self._clean_text_content(sentence)
                        if clean_text and clean_text not in processed_texts:
                            text_info = {
                                'type': 'text',
                                'content': clean_text,
                                'tag': 'p'
                            }
                            content_elements.append(text_info)
                            processed_texts.add(clean_text)
                            logger.info(f"ğŸ“ å¤‡ç”¨æ–¹æ³•å‘ç°æ–‡æœ¬: {clean_text[:50]}...")
        
        # æ–¹æ³•2:å¦‚æœæ–¹æ³•1æ²¡æœ‰ç»“æœ,å°è¯•æ›´å®½æ¾çš„æå–
        if not content_elements:
            logger.info("å°è¯•æ›´å®½æ¾çš„æ–‡æœ¬æå–...")
            # å°è¯•æå–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
            from bs4 import NavigableString
            
            for element in content_elem.descendants:
                if isinstance(element, NavigableString):
                    text = str(element).strip()
                    if text and len(text) > 3:
                        # æ›´å®½æ¾çš„è¿‡æ»¤æ¡ä»¶
                        skip_patterns = ['script', 'style', 'noscript']
                        if not any(pattern in text.lower() for pattern in skip_patterns):
                            clean_text = self._clean_text_content(text)
                            if (clean_text and 
                                clean_text not in processed_texts and
                                len(clean_text) > 3):
                                
                                text_info = {
                                    'type': 'text',
                                    'content': clean_text,
                                    'tag': 'text'
                                }
                                content_elements.append(text_info)
                                processed_texts.add(clean_text)
                                logger.info(f"ğŸ“ å®½æ¾æ¨¡å¼å‘ç°æ–‡æœ¬: {clean_text[:50]}...")
        
        # æ–¹æ³•3:å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœ,ç›´æ¥è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        if not content_elements:
            logger.warning("æ‰€æœ‰æ–‡æœ¬æå–æ–¹æ³•éƒ½å¤±è´¥,è¾“å‡ºè°ƒè¯•ä¿¡æ¯...")
            logger.info(f"å†…å®¹å…ƒç´ HTMLé•¿åº¦: {len(str(content_elem))}")
            logger.info(f"å†…å®¹å…ƒç´ æ–‡æœ¬é•¿åº¦: {len(content_elem.get_text())}")
            # è¾“å‡ºå‰500ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
            sample_text = content_elem.get_text()[:500]
            logger.info(f"å†…å®¹å…ƒç´ æ ·æœ¬æ–‡æœ¬: {sample_text}")
        
        return content_elements
    
    def _is_valid_text_content(self, text, processed_texts):
        """æ£€æŸ¥æ–‡æœ¬å†…å®¹æ˜¯å¦æœ‰æ•ˆ"""
        if not text or len(text) < 3:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
            return False
        
        if text in processed_texts:
            return False
        
        # è¿‡æ»¤æ— ç”¨çš„ç•Œé¢æ–‡å­— - æ›´ç²¾ç¡®çš„è¿‡æ»¤
        filter_patterns = [
            'ç‚¹å‡»å±•å¼€,æŸ¥çœ‹å®Œæ•´å›¾ç‰‡', 'æ”¶èµ·å›å¤', 'æŸ¥çœ‹å…¨éƒ¨', 'æ˜¾ç¤ºå…¨éƒ¨æ¥¼å±‚', 
            'åªçœ‹æ¥¼ä¸»', 'æ¥è‡ª', 'ä½¿ç”¨', 'å®¢æˆ·ç«¯', 'æ›´å¤š', 'APP', 'æ‰‹æœºç‰ˆ',
            'è¯¥æ¥¼å±‚ç–‘ä¼¼è¿è§„', 'éšè—æ­¤æ¥¼', 'æŸ¥çœ‹æ­¤æ¥¼', 'è´´å§', 'ç™¾åº¦', 'ç™»å½•', 'æ³¨å†Œ'
        ]
        
        # å®Œå…¨åŒ¹é…æˆ–åŒ…å«è¿™äº›çŸ­è¯­æ—¶æ‰è¿‡æ»¤
        for pattern in filter_patterns:
            if pattern in text:
                return False
        
        # è¿‡æ»¤çº¯æ•°å­—ã€çº¯é“¾æ¥ã€çº¯ç¬¦å·
        if (text.isdigit() or 
            text.startswith('http') or
            text in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', ',', 'ï¼Œ', 'ï¼›', ';']):
            return False
        
        # è¿‡æ»¤è¿‡çŸ­çš„æ— æ„ä¹‰æ–‡æœ¬
        if len(text.strip()) == 1 and text.strip() in ['å›å¤', 'ä¸¾æŠ¥', 'åˆ é™¤']:
            return False
        
        return True
    
    def _clean_text_content(self, text):
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = ' '.join(text.split())
        
        # å»é™¤HTMLå®ä½“
        import html
        text = html.unescape(text)
        
        # å»é™¤ç‰¹æ®Šå­—ç¬¦å¼€å¤´çš„å†…å®¹
        if text.startswith(('å›å¤', '@', '#')):
            return None
        
        # å»é™¤æœ«å°¾çš„æ— ç”¨å­—ç¬¦
        text = text.rstrip('_-=+')
        
        return text.strip()
    
    def generate_main_markdown(self, post_info, main_post):
        """ç”Ÿæˆä¸»å¸–Markdownå†…å®¹"""
        markdown = f"# {post_info['title']}\n\n"
        markdown += f"**è´´å§**: {post_info['forum_name']}å§\n"
        markdown += f"**ä½œè€…**: {main_post.get('author', 'æœªçŸ¥')}\n\n"
        markdown += "---\n\n"
        
        # ä¸»å¸–å†…å®¹ - æŒ‰åŸå§‹é¡ºåºæ··åˆæ˜¾ç¤º
        if main_post.get('content_elements'):
            img_counter = 1
            for element in main_post['content_elements']:
                if element['type'] == 'text':
                    markdown += f"{element['content']}\n\n"
                elif element['type'] == 'image':
                    markdown += f"![å›¾ç‰‡{img_counter}]({element['src']})\n\n"
                    img_counter += 1
        else:
            # å¤‡ç”¨æ–¹æ¡ˆ
            if main_post.get('content'):
                for paragraph in main_post['content']:
                    markdown += f"{paragraph}\n\n"
            
            if main_post.get('images'):
                for i, img in enumerate(main_post['images'], 1):
                    markdown += f"![å›¾ç‰‡{i}]({img['src']})\n\n"
        
        return markdown
    
    def generate_comments_markdown(self, post_info, replies):
        """ç”Ÿæˆè¯„è®ºMarkdownå†…å®¹"""
        if not replies:
            return ""
        
        markdown = f"# {post_info['title']} - è¯„è®ºåŒº\n\n"
        markdown += f"**åŸå¸–é“¾æ¥**: {post_info['url']}\n\n"
        markdown += "---\n\n"
        
        for reply in replies:
            markdown += f"## {reply['floor']}æ¥¼ - {reply['author']}\n\n"
            
            if reply.get('content_elements'):
                img_counter = 1
                for element in reply['content_elements']:
                    if element['type'] == 'text':
                        markdown += f"{element['content']}\n\n"
                    elif element['type'] == 'image':
                        markdown += f"![{reply['floor']}æ¥¼å›¾ç‰‡{img_counter}]({element['src']})\n\n"
                        img_counter += 1
            else:
                # å¤‡ç”¨æ–¹æ¡ˆ
                if reply.get('content'):
                    for paragraph in reply['content']:
                        markdown += f"{paragraph}\n\n"
                
                if reply.get('images'):
                    for i, img in enumerate(reply['images'], 1):
                        markdown += f"![{reply['floor']}æ¥¼å›¾ç‰‡{i}]({img['src']})\n\n"
            
            markdown += "---\n\n"
        
        return markdown
    
    def update_markdown_with_local_images(self, markdown_content, downloaded_images, post_id):
        """æ›´æ–°Markdownä¸­çš„å›¾ç‰‡é“¾æ¥ä¸ºæœ¬åœ°è·¯å¾„"""
        if not downloaded_images:
            return markdown_content
        
        # åˆ›å»ºURLæ˜ å°„
        url_map = {}
        for img in downloaded_images:
            url_map[img['original_url']] = img['image_url']
        
        # æ›¿æ¢å›¾ç‰‡é“¾æ¥
        for original_url, local_url in url_map.items():
            markdown_content = markdown_content.replace(f"]({original_url})", f"]({local_url})")
        
        return markdown_content


# ============================================================================
# å¾®ä¿¡æŠ“å–ç±» - å®Œå…¨ä¿ç•™åŸæœ‰é€»è¾‘
# ============================================================================
class WeChatArticleScraperAPI:
    def __init__(self, remove_watermarks=True):
        # APIæœåŠ¡å™¨çš„å·¥ä½œç›®å½•
        self.work_dir = os.path.abspath(os.path.dirname(__file__))
        self.static_dir = os.path.join(self.work_dir, 'static', 'wechat')
        self.images_dir = os.path.join(self.static_dir, 'images')
        self.articles_dir = os.path.join(self.static_dir, 'articles')
        
        # å»æ°´å°åŠŸèƒ½å¼€å…³
        self.remove_watermarks = remove_watermarks and HAS_CV2
        
        self.ensure_directories()
        
        if self.remove_watermarks:
            logger.info("ğŸ¨ å·²å¯ç”¨è‡ªåŠ¨å»æ°´å°åŠŸèƒ½")
        elif remove_watermarks and not HAS_CV2:
            logger.warning("âš ï¸ å»æ°´å°åŠŸèƒ½éœ€è¦OpenCV,è¯·è¿è¡Œ: pip install opencv-python")
        else:
            logger.info("ğŸ“· æœªå¯ç”¨å»æ°´å°åŠŸèƒ½")
    
    def ensure_directories(self):
        """ç¡®ä¿æ‰€éœ€ç›®å½•å­˜åœ¨"""
        for directory in [self.static_dir, self.images_dir, self.articles_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)
                logger.info(f"âœ… åˆ›å»ºç›®å½•: {directory}")
    
    def clean_old_files(self, max_age_hours=24):
        """æ¸…ç†è¶…è¿‡æŒ‡å®šæ—¶é—´çš„æ—§æ–‡ä»¶"""
        try:
            current_time = time.time()
            max_age_seconds = max_age_hours * 3600
            
            for directory in [self.images_dir, self.articles_dir]:
                for root, dirs, files in os.walk(directory):
                    for file in files:
                        file_path = os.path.join(root, file)
                        if current_time - os.path.getctime(file_path) > max_age_seconds:
                            try:
                                os.remove(file_path)
                                logger.info(f"ğŸ—‘ï¸ æ¸…ç†æ—§æ–‡ä»¶: {file_path}")
                            except Exception as e:
                                logger.error(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    
                    # æ¸…ç†ç©ºç›®å½•
                    for dir_name in dirs:
                        dir_path = os.path.join(root, dir_name)
                        try:
                            if not os.listdir(dir_path):
                                os.rmdir(dir_path)
                                logger.info(f"ğŸ—‘ï¸ æ¸…ç†ç©ºç›®å½•: {dir_path}")
                        except Exception:
                            pass
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def remove_watermark(self, image_path):
        """å»é™¤å›¾ç‰‡å³ä¸‹è§’æ°´å°"""
        if not HAS_CV2:
            logger.warning(f"âš ï¸ è·³è¿‡å»æ°´å° (OpenCVæœªå®‰è£…): {os.path.basename(image_path)}")
            return False
        
        try:
            # è¯»å–å›¾åƒ
            image = cv2.imread(image_path)
            if image is None:
                logger.error(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {image_path}")
                return False
            
            height, width = image.shape[:2]
            
            # åˆ›å»ºæ©ç ,æ ‡è®°å³ä¸‹è§’æ°´å°åŒºåŸŸ
            mask = np.zeros(image.shape[:2], np.uint8)
            
            # æ ¹æ®å›¾ç‰‡å¤§å°åŠ¨æ€è°ƒæ•´æ°´å°åŒºåŸŸ
            watermark_width = min(420, int(width * 0.3))
            watermark_height = min(50, int(height * 0.1))
            
            # ç»˜åˆ¶çŸ©å½¢æ©ç æ ‡è®°æ°´å°åŒºåŸŸ
            cv2.rectangle(mask, 
                         (width - watermark_width, height - watermark_height), 
                         (width, height), 
                         255, -1)
            
            # ä½¿ç”¨inpaintå‡½æ•°ä¿®å¤å›¾åƒ,å»é™¤æ°´å°
            denoised_image = cv2.inpaint(image, mask, 3, cv2.INPAINT_TELEA)
            
            # ä¿å­˜å¤„ç†åçš„å›¾åƒ,è¦†ç›–åŸæ–‡ä»¶
            success = cv2.imwrite(image_path, denoised_image)
            
            if success:
                logger.info(f"âœ… å·²å»æ°´å°: {os.path.basename(image_path)}")
                return True
            else:
                logger.error(f"âŒ ä¿å­˜å»æ°´å°å›¾ç‰‡å¤±è´¥: {os.path.basename(image_path)}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å»æ°´å°å¤„ç†å¤±è´¥ {os.path.basename(image_path)}: {e}")
            return False
    
    def extract_article_id(self, url):
        """ä»å¾®ä¿¡é“¾æ¥ä¸­æå–æ–‡ç« ID"""
        try:
            parsed = urlparse(url)
            if 'mp.weixin.qq.com' in parsed.netloc:
                params = parse_qs(parsed.query)
                return {
                    'url': url,
                    'domain': parsed.netloc,
                    'params': params
                }
            return None
        except Exception as e:
            logger.error(f"âŒ è§£æURLå¤±è´¥: {e}")
            return None
    
    def download_images(self, images, article_id):
        """ä¸‹è½½æ–‡ç« ä¸­çš„å›¾ç‰‡åˆ°é™æ€ç›®å½•"""
        if not images:
            return []
        
        # åˆ›å»ºæ–‡ç« ä¸“å±çš„å›¾ç‰‡ç›®å½•
        article_images_dir = os.path.join(self.images_dir, article_id)
        if not os.path.exists(article_images_dir):
            os.makedirs(article_images_dir)
        
        downloaded_images = []
        
        for i, img in enumerate(images, 1):
            try:
                img_url = img['src']
                if not img_url:
                    continue
                
                # ç¡®å®šæ–‡ä»¶æ‰©å±•å
                if 'jpeg' in img_url.lower() or 'jpg' in img_url.lower():
                    ext = '.jpg'
                elif 'png' in img_url.lower():
                    ext = '.png'
                elif 'gif' in img_url.lower():
                    ext = '.gif'
                elif 'webp' in img_url.lower():
                    ext = '.webp'
                else:
                    ext = '.jpg'  # é»˜è®¤
                
                # ç”Ÿæˆæ–‡ä»¶å
                img_filename = f"image_{i:03d}{ext}"
                img_filepath = os.path.join(article_images_dir, img_filename)
                
                # ä¸‹è½½å›¾ç‰‡
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://mp.weixin.qq.com/'
                }
                
                req = urllib.request.Request(img_url, headers=headers)
                with urllib.request.urlopen(req, timeout=10) as response:
                    if response.status == 200:
                        with open(img_filepath, 'wb') as f:
                            f.write(response.read())
                        
                        # ä¸‹è½½å®Œæˆåè‡ªåŠ¨å»æ°´å°(å¦‚æœå¯ç”¨)
                        watermark_removed = False
                        if self.remove_watermarks:
                            watermark_removed = self.remove_watermark(img_filepath)
                        
                        # ç”Ÿæˆå¯è®¿é—®çš„URL
                        image_url = f"/weixin/images/{article_id}/{img_filename}"
                        
                        downloaded_images.append({
                            'original_url': img_url,
                            'local_path': img_filepath,
                            'filename': img_filename,
                            'image_url': image_url,  # ä¾›å‰ç«¯è®¿é—®çš„URL
                            'alt': img.get('alt', ''),
                            'title': img.get('title', ''),
                            'watermark_removed': watermark_removed
                        })
                        logger.info(f"ğŸ“· å·²ä¸‹è½½å›¾ç‰‡ {i}: {img_filename}")
                    else:
                        logger.warning(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ {i}: HTTP {response.status}")
                        
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½å›¾ç‰‡ {i} å¤±è´¥: {e}")
                continue
        
        return downloaded_images
    
    def scrape_wechat_article(self, article_url):
        """æŠ“å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹"""
        logger.info(f"ğŸ” å¼€å§‹æŠ“å–å¾®ä¿¡æ–‡ç« : {article_url}")
        
        # é¦–å…ˆå°è¯•ç”¨requestsç®€å•è·å–
        try:
            logger.info("ğŸ“„ é¢„æ£€æŸ¥ç½‘é¡µå¯è®¿é—®æ€§...")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            response = requests.get(article_url, headers=headers, timeout=10)
            logger.info(f"ğŸ“Š HTTPçŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                if "ç¯å¢ƒå¼‚å¸¸" in response.text or "å®ŒæˆéªŒè¯" in response.text:
                    logger.warning("âš ï¸ æ£€æµ‹åˆ°éœ€è¦éªŒè¯,å°è¯•æµè§ˆå™¨æ–¹å¼...")
                elif len(response.text) < 1000:
                    logger.warning("âš ï¸ å†…å®¹è¿‡å°‘,å¯èƒ½è¢«æ‹¦æˆª,å°è¯•æµè§ˆå™¨æ–¹å¼...")
                else:
                    logger.info("âœ… é¢„æ£€æŸ¥é€šè¿‡,å°è¯•ç›´æ¥è§£æHTML...")
                    return self.parse_html_content(response.text, article_url)
            else:
                logger.warning(f"âš ï¸ HTTPçŠ¶æ€ç å¼‚å¸¸: {response.status_code},å°è¯•æµè§ˆå™¨æ–¹å¼...")
                
        except Exception as e:
            logger.warning(f"âš ï¸ é¢„æ£€æŸ¥å¤±è´¥: {e},å°è¯•æµè§ˆå™¨æ–¹å¼...")
        
        # ä½¿ç”¨æµè§ˆå™¨æ–¹å¼
        return self.scrape_with_browser(article_url)
    
    def parse_html_content(self, html_content, article_url):
        """ç›´æ¥è§£æHTMLå†…å®¹"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_elem = (soup.find('h1', {'id': 'activity-name'}) or 
                         soup.find('h1', class_='rich_media_title') or
                         soup.find('h1'))
            
            article_title = title_elem.get_text().strip() if title_elem else "æœªæ‰¾åˆ°æ ‡é¢˜"
            
            # æå–ä½œè€…
            author_elem = (soup.find('span', {'id': 'js_name'}) or
                          soup.find('span', class_='profile_nickname'))
            author_name = author_elem.get_text().strip() if author_elem else "æœªçŸ¥ä½œè€…"
            
            # æå–å‘å¸ƒæ—¶é—´
            time_elem = soup.find('span', {'id': 'publish_time'})
            publish_time = time_elem.get_text().strip() if time_elem else "æœªçŸ¥æ—¶é—´"
            
            # æå–æ­£æ–‡å†…å®¹ - ä¿æŒåŸå§‹æ’ç‰ˆé¡ºåº
            content_elem = (soup.find('div', {'id': 'js_content'}) or
                           soup.find('div', class_='rich_media_content'))
            
            content_elements = []
            images = []
            
            if content_elem:
                processed_texts = set()
                
                for element in content_elem.find_all(recursive=True):
                    # å¤„ç†å›¾ç‰‡
                    if element.name == 'img':
                        img_src = element.get('src') or element.get('data-src')
                        if img_src:
                            # å¤„ç†å¾®ä¿¡å›¾ç‰‡URL
                            if img_src.startswith('//'):
                                img_src = 'https:' + img_src
                            elif img_src.startswith('/'):
                                img_src = 'https://mp.weixin.qq.com' + img_src
                            
                            img_already_added = any(
                                el['type'] == 'image' and el['src'] == img_src 
                                for el in content_elements
                            )
                            
                            if not img_already_added:
                                img_info = {
                                    'type': 'image',
                                    'src': img_src,
                                    'alt': element.get('alt', ''),
                                    'title': element.get('title', ''),
                                    'order': len(content_elements)
                                }
                                content_elements.append(img_info)
                                images.append(img_info)
                                logger.info(f"ğŸ“· å‘ç°å›¾ç‰‡: {img_src}")
                    
                    # å¤„ç†æ–‡æœ¬å†…å®¹
                    elif element.name in ['p', 'div', 'section', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                        has_block_children = bool(element.find(['p', 'div', 'section', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']))
                        
                        if has_block_children and element.name in ['div', 'section']:
                            continue
                        
                        if element.find('img'):
                            continue
                            
                        text = element.get_text().strip()
                        if (text and 
                            len(text) > 10 and
                            text not in processed_texts and
                            not text.startswith('http') and
                            'é˜…è¯»åŸæ–‡' not in text and
                            'ç‚¹å‡»æŸ¥çœ‹' not in text):
                            
                            is_substring_of_existing = any(
                                text in existing_text or existing_text in text
                                for existing_text in processed_texts
                                if len(existing_text) > len(text)
                            )
                            
                            if not is_substring_of_existing:
                                text_info = {
                                    'type': 'text',
                                    'content': text,
                                    'tag': element.name,
                                    'order': len(content_elements)
                                }
                                content_elements.append(text_info)
                                processed_texts.add(text)
            
            # æŒ‰orderæ’åºç¡®ä¿é¡ºåºæ­£ç¡®
            content_elements.sort(key=lambda x: x['order'])
            
            # ç»Ÿè®¡ä¿¡æ¯
            text_count = len([elem for elem in content_elements if elem['type'] == 'text'])
            image_count = len([elem for elem in content_elements if elem['type'] == 'image'])
            
            logger.info(f"âœ… HTMLè§£ææˆåŠŸ: {text_count} ä¸ªæ–‡æœ¬æ®µè½, {image_count} å¼ å›¾ç‰‡")
            
            return {
                'success': True,
                'title': article_title,
                'author': author_name,
                'publish_time': publish_time,
                'content_elements': content_elements,
                'content': [elem['content'] for elem in content_elements if elem['type'] == 'text'],
                'images': images,
                'paragraph_count': text_count,
                'image_count': image_count,
                'total_elements': len(content_elements),
                'error': None,
                'method': 'html_parsing'
            }
            
        except Exception as e:
            logger.error(f"âŒ HTMLè§£æå¤±è´¥: {e}")
            return self.scrape_with_browser(article_url)
    
    def scrape_with_browser(self, article_url):
        """ä½¿ç”¨æµè§ˆå™¨æŠ“å–"""
        logger.info("ğŸŒ å¯åŠ¨æµè§ˆå™¨è¿›è¡ŒæŠ“å–...")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-extensions',
                    '--disable-plugins',
                    '--disable-images',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--single-process',
                ]
            )
            
            context = browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) "
                    "AppleWebKit/605.1.15 (KHTML, like Gecko) "
                    "Version/14.0 Mobile/15E148 Safari/604.1"
                ),
                viewport={"width": 375, "height": 667},
            )
            
            page = context.new_page()
            
            try:
                logger.info("ğŸŒ æ­£åœ¨è®¿é—®æ–‡ç« é¡µé¢...")
                page.set_default_timeout(30000)
                
                response = page.goto(article_url, wait_until="domcontentloaded")
                logger.info(f"ğŸ“„ é¡µé¢å“åº”çŠ¶æ€: {response.status}")
                
                time.sleep(5)
                
                # æ£€æŸ¥éªŒè¯
                page_content = page.content()
                if "ç¯å¢ƒå¼‚å¸¸" in page_content or "å®ŒæˆéªŒè¯" in page_content:
                    logger.warning("âš ï¸ æ£€æµ‹åˆ°éœ€è¦éªŒè¯,ç­‰å¾…å¤„ç†...")
                    time.sleep(10)
                
                # æå–å†…å®¹
                article_data = self.extract_article_content_from_page(page)
                article_data['url'] = article_url
                article_data['extraction_time'] = datetime.now().isoformat()
                
                return article_data
                
            except Exception as e:
                logger.error(f"âŒ æµè§ˆå™¨æŠ“å–å¤±è´¥: {e}")
                return {
                    'success': False,
                    'error': str(e),
                    'url': article_url,
                    'extraction_time': datetime.now().isoformat()
                }
            finally:
                browser.close()
    
    def extract_article_content_from_page(self, page):
        """ä»é¡µé¢æå–æ–‡ç« å†…å®¹(æµè§ˆå™¨ç‰ˆæœ¬)"""
        try:
            # å„ç§é€‰æ‹©å™¨
            title_selectors = ['#activity-name', '.rich_media_title', 'h1']
            content_selectors = ['#js_content', '.rich_media_content', 'article']
            date_selectors = ['#publish_time', '.publish_time']
            author_selectors = ['#js_name', '.profile_nickname']
            
            # æå–æ ‡é¢˜
            article_title = "æœªæ‰¾åˆ°æ ‡é¢˜"
            for selector in title_selectors:
                try:
                    title_elem = page.query_selector(selector)
                    if title_elem:
                        title_text = title_elem.inner_text().strip()
                        if title_text and len(title_text) > 3:
                            article_title = title_text
                            break
                except:
                    continue
            
            # æå–ä½œè€…
            author_name = "æœªçŸ¥ä½œè€…"
            for selector in author_selectors:
                try:
                    author_elem = page.query_selector(selector)
                    if author_elem:
                        author_text = author_elem.inner_text().strip()
                        if author_text and len(author_text) > 1:
                            author_name = author_text
                            break
                except:
                    continue
            
            # æå–å‘å¸ƒæ—¶é—´
            publish_time = "æœªçŸ¥æ—¶é—´"
            for selector in date_selectors:
                try:
                    date_elem = page.query_selector(selector)
                    if date_elem:
                        date_text = date_elem.inner_text().strip()
                        if date_text and len(date_text) > 3:
                            publish_time = date_text
                            break
                except:
                    continue
            
            # æå–å†…å®¹
            content_elements = []
            images = []
            content_found = False
            
            for selector in content_selectors:
                try:
                    content_elem = page.query_selector(selector)
                    if content_elem:
                        logger.info(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨æ‰¾åˆ°å†…å®¹: {selector}")
                        
                        # ç®€åŒ–çš„å†…å®¹æå–
                        # æå–æ‰€æœ‰æ–‡æœ¬æ®µè½
                        paragraphs = page.query_selector_all(f"{selector} p, {selector} div")
                        for i, para in enumerate(paragraphs):
                            try:
                                text = para.inner_text().strip()
                                if text and len(text) > 10 and 'é˜…è¯»åŸæ–‡' not in text:
                                    content_elements.append({
                                        'type': 'text',
                                        'content': text,
                                        'tag': 'p',
                                        'order': len(content_elements)
                                    })
                            except:
                                continue
                        
                        # æå–æ‰€æœ‰å›¾ç‰‡
                        img_elements = page.query_selector_all(f"{selector} img")
                        for img_elem in img_elements:
                            try:
                                img_src = img_elem.get_attribute('src') or img_elem.get_attribute('data-src')
                                if img_src:
                                    if img_src.startswith('//'):
                                        img_src = 'https:' + img_src
                                    elif img_src.startswith('/'):
                                        img_src = 'https://mp.weixin.qq.com' + img_src
                                    
                                    img_info = {
                                        'type': 'image',
                                        'src': img_src,
                                        'alt': img_elem.get_attribute('alt') or '',
                                        'title': img_elem.get_attribute('title') or '',
                                        'order': len(content_elements)
                                    }
                                    content_elements.append(img_info)
                                    images.append(img_info)
                            except:
                                continue
                        
                        if content_elements:
                            content_found = True
                            break
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ é€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                    continue
            
            # æ’åºç¡®ä¿æ­£ç¡®é¡ºåº
            content_elements.sort(key=lambda x: x['order'])
            
            # ç»Ÿè®¡ä¿¡æ¯
            text_count = len([elem for elem in content_elements if elem['type'] == 'text'])
            image_count = len([elem for elem in content_elements if elem['type'] == 'image'])
            
            logger.info(f"ğŸ“ æå–åˆ° {text_count} ä¸ªæ–‡æœ¬æ®µè½, {image_count} å¼ å›¾ç‰‡")
            
            return {
                'success': True,
                'title': article_title,
                'author': author_name,
                'publish_time': publish_time,
                'content_elements': content_elements,
                'content': [elem['content'] for elem in content_elements if elem['type'] == 'text'],
                'images': images,
                'paragraph_count': text_count,
                'image_count': image_count,
                'total_elements': len(content_elements),
                'error': None,
                'method': 'browser_extraction'
            }
            
        except Exception as e:
            logger.error(f"âŒ å†…å®¹æå–å¤±è´¥: {e}")
            return {
                'success': False,
                'title': "æå–å¤±è´¥",
                'author': "æœªçŸ¥",
                'publish_time': "æœªçŸ¥",
                'content_elements': [],
                'content': [],
                'images': [],
                'paragraph_count': 0,
                'image_count': 0,
                'total_elements': 0,
                'error': str(e),
                'method': 'browser_extraction'
            }
    
    def generate_markdown(self, article_data, downloaded_images=None):
        """ç”ŸæˆMarkdownå†…å®¹,ä½¿ç”¨æœ¬åœ°å›¾ç‰‡URL"""
        markdown_content = f"# {article_data.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n\n"
        
        # åˆ›å»ºå›¾ç‰‡URLæ˜ å°„
        image_url_map = {}
        if downloaded_images:
            for img in downloaded_images:
                image_url_map[img['original_url']] = img['image_url']
        
        # æŒ‰åŸå§‹é¡ºåºæ··åˆæ˜¾ç¤ºæ–‡æœ¬å’Œå›¾ç‰‡
        img_counter = 1
        for element in article_data.get('content_elements', []):
            if element['type'] == 'text':
                # æ ¹æ®æ ‡ç­¾ç±»å‹æ·»åŠ é€‚å½“çš„markdownæ ¼å¼
                if element['tag'] in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    level = int(element['tag'][1])
                    markdown_content += f"{'#' * (level + 1)} {element['content']}\n\n"
                else:
                    markdown_content += f"{element['content']}\n\n"
            elif element['type'] == 'image':
                # ä½¿ç”¨æœ¬åœ°å›¾ç‰‡URLæˆ–åŸå§‹URL
                img_url = image_url_map.get(element['src'], element['src'])
                markdown_content += f"![å›¾ç‰‡{img_counter}]({img_url})\n"
                if element['alt']:
                    markdown_content += f"*{element['alt']}*\n"
                markdown_content += "\n"
                img_counter += 1
        
        return markdown_content


# åˆ›å»ºå…¨å±€å®ä¾‹
tieba_scraper = TiebaPostScraperAPI()
wechat_scraper = WeChatArticleScraperAPI()


# ============================================================================
# Flaskè·¯ç”±
# ============================================================================

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'unified-content-scraper-api',
        'endpoints': {
            'tieba': '/tieba/scrape',
            'wechat': '/weixin/scrape'
        }
    })


@app.route('/tieba/scrape', methods=['POST'])
def scrape_tieba():
    """æŠ“å–è´´å§å¸–å­æ¥å£"""
    try:
        # è·å–è¯·æ±‚å‚æ•°
        data = request.get_json()
        if not data or 'url' not in data:
            return jsonify({
                'success': False,
                'error': 'Missing required parameter: url'
            }), 400
        
        post_url = data['url']
        download_images = data.get('download_images', True)
        
        logger.info(f"ğŸš€ æ”¶åˆ°è´´å§æŠ“å–è¯·æ±‚: {post_url}")
        
        # æ¸…ç†æ—§æ–‡ä»¶
        tieba_scraper.clean_old_files()
        
        # éªŒè¯URL
        if 'tieba.baidu.com' not in post_url:
            return jsonify({
                'success': False,
                'error': 'Invalid Tieba post URL'
            }), 400
        
        # æŠ“å–å¸–å­
        post_data = tieba_scraper.scrape_tieba_post(post_url)
        
        if not post_data.get('success'):
            return jsonify(post_data), 500
        
        # ç”Ÿæˆå¸–å­ID
        post_id = post_data.get('post_info', {}).get('post_id', 'unknown')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_title = re.sub(r'[^\w\s-]', '', post_data.get('post_info', {}).get('title', 'post'))
        safe_title = re.sub(r'[-\s]+', '-', safe_title)[:30]
        unique_id = f"{safe_title}_{post_id}_{timestamp}"
        
        # ä¸‹è½½å›¾ç‰‡
        downloaded_images = []
        if download_images and post_data.get('all_images'):
            logger.info(f"ğŸ“· å¼€å§‹ä¸‹è½½ {len(post_data['all_images'])} å¼ å›¾ç‰‡...")
            downloaded_images = tieba_scraper.download_images(post_data['all_images'], unique_id)
            logger.info(f"âœ… æˆåŠŸä¸‹è½½ {len(downloaded_images)} å¼ å›¾ç‰‡")
        
        # ç”ŸæˆMarkdownå†…å®¹(ä½¿ç”¨æœ¬åœ°å›¾ç‰‡URL)
        main_markdown = post_data.get('main_markdown', '')
        comments_markdown = post_data.get('comments_markdown', '')
        
        # æ›´æ–°å›¾ç‰‡é“¾æ¥ä¸ºæœ¬åœ°URL
        if downloaded_images:
            main_markdown = tieba_scraper.update_markdown_with_local_images(
                main_markdown, downloaded_images, unique_id)
            comments_markdown = tieba_scraper.update_markdown_with_local_images(
                comments_markdown, downloaded_images, unique_id)
        
        # ä¿å­˜å¸–å­JSONåˆ°é™æ€ç›®å½•
        post_filename = f"{unique_id}.json"
        post_filepath = os.path.join(tieba_scraper.posts_dir, post_filename)
        
        # æ„å»ºå®Œæ•´çš„å“åº”æ•°æ®
        response_data = {
            'success': True,
            'post_id': unique_id,
            'title': post_data.get('post_info', {}).get('title'),
            'forum_name': post_data.get('post_info', {}).get('forum_name'),
            'author': post_data.get('main_post', {}).get('author'),
            'total_replies': post_data.get('total_replies', 0),
            'image_count': len(downloaded_images),
            'total_images': post_data.get('total_images', 0),
            'main_markdown': main_markdown,  # n8nä½¿ç”¨çš„ä¸»å¸–å†…å®¹
            'comments_markdown': comments_markdown,  # è¯„è®ºå†…å®¹
            'extraction_time': datetime.now().isoformat(),
            'method': post_data.get('method'),
            'source_url': post_url,
            'images': []  # å›¾ç‰‡è®¿é—®ä¿¡æ¯
        }
        
        # æ·»åŠ å›¾ç‰‡è®¿é—®ä¿¡æ¯ - ä½¿ç”¨åŠ¨æ€URL
        base_url = request.host_url.rstrip('/')
        for img in downloaded_images:
            response_data['images'].append({
                'filename': img['filename'],
                'url': f"{base_url}{img['image_url']}",  # åŠ¨æ€æ‹¼æ¥å®Œæ•´è®¿é—®URL
                'alt': img['alt'],
                'title': img['title'],
                'watermark_removed': img['watermark_removed']
            })
        
        # ä¿å­˜å®Œæ•´æ•°æ®åˆ°JSONæ–‡ä»¶
        full_data = {
            **response_data,
            'post_info': post_data.get('post_info', {}),
            'main_post': post_data.get('main_post', {}),
            'replies': post_data.get('replies', []),
        }
        
        with open(post_filepath, 'w', encoding='utf-8') as f:
            json.dump(full_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… æŠ“å–å®Œæˆ: {post_data.get('post_info', {}).get('title')}")
        logger.info(f"ğŸ“ JSONæ–‡ä»¶: {post_filepath}")
        logger.info(f"ğŸ“ ä¸»å¸–Markdowné•¿åº¦: {len(main_markdown)} å­—ç¬¦")
        logger.info(f"ğŸ“ è¯„è®ºMarkdowné•¿åº¦: {len(comments_markdown)} å­—ç¬¦")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"âŒ æŠ“å–å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/weixin/scrape', methods=['POST'])
def scrape_wechat():
    """æŠ“å–å¾®ä¿¡æ–‡ç« æ¥å£"""
    try:
        # è·å–è¯·æ±‚å‚æ•°
        data = request.get_json()
        if not data or 'url' not in data:
            return jsonify({
                'success': False,
                'error': 'Missing required parameter: url'
            }), 400
        
        article_url = data['url']
        download_images = data.get('download_images', True)
        
        logger.info(f"ğŸš€ æ”¶åˆ°å¾®ä¿¡æŠ“å–è¯·æ±‚: {article_url}")
        
        # æ¸…ç†æ—§æ–‡ä»¶
        wechat_scraper.clean_old_files()
        
        # éªŒè¯URL
        if 'mp.weixin.qq.com' not in article_url:
            return jsonify({
                'success': False,
                'error': 'Invalid WeChat article URL'
            }), 400
        
        # æŠ“å–æ–‡ç« 
        article_data = wechat_scraper.scrape_wechat_article(article_url)
        
        if not article_data.get('success'):
            return jsonify(article_data), 500
        
        # ç”Ÿæˆæ–‡ç« ID(ç”¨äºæ–‡ä»¶ç»„ç»‡)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_title = re.sub(r'[^\w\s-]', '', article_data.get('title', 'article'))
        safe_title = re.sub(r'[-\s]+', '-', safe_title)[:30]
        article_id = f"{safe_title}_{timestamp}"
        
        # ä¸‹è½½å›¾ç‰‡
        downloaded_images = []
        if download_images and article_data.get('images'):
            logger.info(f"ğŸ“· å¼€å§‹ä¸‹è½½ {len(article_data['images'])} å¼ å›¾ç‰‡...")
            downloaded_images = wechat_scraper.download_images(article_data['images'], article_id)
            logger.info(f"âœ… æˆåŠŸä¸‹è½½ {len(downloaded_images)} å¼ å›¾ç‰‡")
        
        # ç”ŸæˆMarkdownå†…å®¹(ä½¿ç”¨æœ¬åœ°å›¾ç‰‡URL)
        markdown_content = wechat_scraper.generate_markdown(article_data, downloaded_images)
        
        # ä¿å­˜æ–‡ç« JSONåˆ°é™æ€ç›®å½•
        article_filename = f"{article_id}.json"
        article_filepath = os.path.join(wechat_scraper.articles_dir, article_filename)
        
        # æ„å»ºå®Œæ•´çš„å“åº”æ•°æ®
        response_data = {
            'success': True,
            'article_id': article_id,
            'title': article_data.get('title'),
            'author': article_data.get('author'),
            'publish_time': article_data.get('publish_time'),
            'paragraph_count': article_data.get('paragraph_count', 0),
            'image_count': len(downloaded_images),
            'total_elements': article_data.get('total_elements', 0),
            'markdown': markdown_content,  # è¿™æ˜¯n8néœ€è¦çš„å…³é”®å­—æ®µ
            'extraction_time': datetime.now().isoformat(),
            'method': article_data.get('method'),
            'source_url': article_url,
            'images': []  # å›¾ç‰‡è®¿é—®ä¿¡æ¯
        }
        
        # æ·»åŠ å›¾ç‰‡è®¿é—®ä¿¡æ¯ - ä½¿ç”¨åŠ¨æ€URL
        base_url = request.host_url.rstrip('/')
        for img in downloaded_images:
            response_data['images'].append({
                'filename': img['filename'],
                'url': f"{base_url}{img['image_url']}",  # åŠ¨æ€æ‹¼æ¥å®Œæ•´è®¿é—®URL
                'alt': img['alt'],
                'title': img['title'],
                'watermark_removed': img['watermark_removed']
            })
        
        # ä¿å­˜å®Œæ•´æ•°æ®åˆ°JSONæ–‡ä»¶
        full_data = {
            **response_data,
            'content_elements': article_data.get('content_elements', []),
            'raw_content': article_data.get('content', []),
        }
        
        with open(article_filepath, 'w', encoding='utf-8') as f:
            json.dump(full_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… æŠ“å–å®Œæˆ: {article_data.get('title')}")
        logger.info(f"ğŸ“ JSONæ–‡ä»¶: {article_filepath}")
        logger.info(f"ğŸ“ Markdowné•¿åº¦: {len(markdown_content)} å­—ç¬¦")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"âŒ æŠ“å–å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/tieba/images/<path:filename>')
def serve_tieba_image(filename):
    """æä¾›è´´å§å›¾ç‰‡é™æ€æ–‡ä»¶æœåŠ¡"""
    try:
        return send_from_directory(tieba_scraper.images_dir, filename)
    except Exception as e:
        logger.error(f"âŒ å›¾ç‰‡æœåŠ¡å¤±è´¥: {e}")
        return jsonify({'error': 'Image not found'}), 404


@app.route('/weixin/images/<path:filename>')
def serve_wechat_image(filename):
    """æä¾›å¾®ä¿¡å›¾ç‰‡é™æ€æ–‡ä»¶æœåŠ¡"""
    try:
        return send_from_directory(wechat_scraper.images_dir, filename)
    except Exception as e:
        logger.error(f"âŒ å›¾ç‰‡æœåŠ¡å¤±è´¥: {e}")
        return jsonify({'error': 'Image not found'}), 404


@app.route('/tieba/posts/<filename>')
def serve_tieba_post(filename):
    """æä¾›è´´å§å¸–å­JSONæ–‡ä»¶æœåŠ¡"""
    try:
        return send_from_directory(tieba_scraper.posts_dir, filename)
    except Exception as e:
        logger.error(f"âŒ å¸–å­æ–‡ä»¶æœåŠ¡å¤±è´¥: {e}")
        return jsonify({'error': 'Post not found'}), 404


@app.route('/weixin/articles/<filename>')
def serve_wechat_article(filename):
    """æä¾›å¾®ä¿¡æ–‡ç« JSONæ–‡ä»¶æœåŠ¡"""
    try:
        return send_from_directory(wechat_scraper.articles_dir, filename)
    except Exception as e:
        logger.error(f"âŒ æ–‡ç« æ–‡ä»¶æœåŠ¡å¤±è´¥: {e}")
        return jsonify({'error': 'Article not found'}), 404


@app.route('/tieba/list', methods=['GET'])
def list_tieba_posts():
    """åˆ—å‡ºæ‰€æœ‰å·²æŠ“å–çš„è´´å§å¸–å­"""
    try:
        posts = []
        for filename in os.listdir(tieba_scraper.posts_dir):
            if filename.endswith('.json'):
                filepath = os.path.join(tieba_scraper.posts_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        post_data = json.load(f)
                        posts.append({
                            'filename': filename,
                            'post_id': post_data.get('post_id'),
                            'title': post_data.get('title'),
                            'extraction_time': post_data.get('extraction_time'),
                            'image_count': post_data.get('image_count', 0),
                            'total_replies': post_data.get('total_replies', 0)
                        })
                except Exception as e:
                    logger.error(f"è¯»å–å¸–å­æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                    continue
        
        # æŒ‰æ—¶é—´å€’åºæ’åº
        posts.sort(key=lambda x: x.get('extraction_time', ''), reverse=True)
        
        return jsonify({
            'success': True,
            'count': len(posts),
            'posts': posts
        })
        
    except Exception as e:
        logger.error(f"âŒ è·å–å¸–å­åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/weixin/list', methods=['GET'])
def list_wechat_articles():
    """åˆ—å‡ºæ‰€æœ‰å·²æŠ“å–çš„å¾®ä¿¡æ–‡ç« """
    try:
        articles = []
        for filename in os.listdir(wechat_scraper.articles_dir):
            if filename.endswith('.json'):
                filepath = os.path.join(wechat_scraper.articles_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        article_data = json.load(f)
                        articles.append({
                            'filename': filename,
                            'article_id': article_data.get('article_id'),
                            'title': article_data.get('title'),
                            'author': article_data.get('author'),
                            'extraction_time': article_data.get('extraction_time'),
                            'image_count': article_data.get('image_count', 0),
                            'paragraph_count': article_data.get('paragraph_count', 0)
                        })
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ç« æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                    continue
        
        # æŒ‰æ—¶é—´å€’åºæ’åº
        articles.sort(key=lambda x: x.get('extraction_time', ''), reverse=True)
        
        return jsonify({
            'success': True,
            'count': len(articles),
            'articles': articles
        })
        
    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/clean', methods=['POST'])
def clean_old_files():
    """æ‰‹åŠ¨æ¸…ç†æ—§æ–‡ä»¶æ¥å£"""
    try:
        data = request.get_json() or {}
        max_age_hours = data.get('max_age_hours', 24)
        
        tieba_scraper.clean_old_files(max_age_hours)
        wechat_scraper.clean_old_files(max_age_hours)
        
        return jsonify({
            'success': True,
            'message': f'å·²æ¸…ç†è¶…è¿‡ {max_age_hours} å°æ—¶çš„æ—§æ–‡ä»¶',
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ æ¸…ç†æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'Not found'}), 404


@app.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'Internal server error'}), 500


def run_server(host='0.0.0.0', port=7000, debug=False):
    """å¯åŠ¨FlaskæœåŠ¡å™¨"""
    print("ğŸš€ ç»Ÿä¸€å†…å®¹æŠ“å–APIæœåŠ¡å™¨")
    print("=" * 60)
    print(f"ğŸ“¡ æœåŠ¡åœ°å€: http://{host}:{port}")
    print(f"ğŸ“ è´´å§é™æ€ç›®å½•: {tieba_scraper.static_dir}")
    print(f"ğŸ“ å¾®ä¿¡é™æ€ç›®å½•: {wechat_scraper.static_dir}")
    print("=" * 60)
    print("APIæ¥å£:")
    print(f"  POST /tieba/scrape         - æŠ“å–è´´å§å¸–å­")
    print(f"  POST /weixin/scrape        - æŠ“å–å¾®ä¿¡æ–‡ç« ")
    print(f"  GET  /health               - å¥åº·æ£€æŸ¥")
    print(f"  GET  /tieba/list           - è´´å§å¸–å­åˆ—è¡¨")
    print(f"  GET  /weixin/list          - å¾®ä¿¡æ–‡ç« åˆ—è¡¨")
    print(f"  POST /clean                - æ¸…ç†æ—§æ–‡ä»¶")
    print("=" * 60)
    print("n8nè°ƒç”¨ç¤ºä¾‹:")
    print(f"  # è´´å§")
    print(f"  POST http://localhost:{port}/tieba/scrape")
    print(f'  Body: {{"url": "https://tieba.baidu.com/p/123456"}}')
    print(f"  # å¾®ä¿¡")
    print(f"  POST http://localhost:{port}/weixin/scrape")
    print(f'  Body: {{"url": "https://mp.weixin.qq.com/s/xxx"}}')
    print("=" * 60)
    
    app.run(host=host, port=port, debug=debug, threaded=True)


if __name__ == '__main__':
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    host = '0.0.0.0'
    port = int(os.environ.get('PORT', 7000))  # ğŸ”§ ä¿®æ”¹1: ä»ç¯å¢ƒå˜é‡è¯»å–ç«¯å£
    debug = False
    
    for arg in sys.argv[1:]:
        if arg.startswith('--host='):
            host = arg.split('=', 1)[1]
        elif arg.startswith('--port='):
            port = int(arg.split('=', 1)[1])
        elif arg == '--debug':
            debug = True
        elif arg == '--help':
            print("ä½¿ç”¨æ–¹æ³•:")
            print("  python unified_scraper_api.py [é€‰é¡¹]")
            print("é€‰é¡¹:")
            print("  --host=HOST     æœåŠ¡å™¨åœ°å€ (é»˜è®¤: 0.0.0.0)")
            print("  --port=PORT     ç«¯å£å· (é»˜è®¤: ä»ç¯å¢ƒå˜é‡PORTè¯»å–,å¦åˆ™7000)")
            print("  --debug         å¯ç”¨è°ƒè¯•æ¨¡å¼")
            print("  --help          æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
            sys.exit(0)
    
    # å¯åŠ¨æœåŠ¡å™¨
    run_server(host=host, port=port, debug=debug)
